~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*  Machine Learning Pipe Line
*  LGBM 
*  Boosting(gradient & adaptive)
*  bias-variance trade-off와 머신러닝 모델링의 목적
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# Machine Learning Pipe Line

1.  Data Retrieval
2.  Data Preparation
3.  Modeling
4.  Model Evaluation
5.  Prediction

# LGBM & Boosting

  * LGBM<br>
    학습을 하는데 tree를 쓰는 gradient boosting framework <br>

    * Boosting(부스팅?)<br>
      약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법<br>
      
        * gradient boosting?<br>
          이전까지의 오차를 보정하도록 예측기를 순차적으로 추가(앙상블 시킨다?)<br>
          반복할 때 마다, 그 이전 예측기가 만든 '잔여 오차'(loss) 에 새로운 예측기를 학습시킨다.<br>

        * Adaptive boosting<br>
          이전 학습기에서 잘 학습되지 않은 샘플의 가중치를 높여서 다음 학습기가 '그 샘플'에 더 잘 훈련할수 있도록 하는 알고리즘 <br>
          보통 SVM 과 잘 쓰지 않는다<br>
           ->  SVM 의 속도가 느리고, 아다부스트와 함꼐 사용했을 떄 불안정<br>
            <br>
        * Gradient Descent Algorithm<br>
          여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 '최적화'알고리즘<br>
          목적 : '비용함수(Cost Function)'을 최소화 <br>
                        -> 파라미터 조정<br>
  
         * Gradient Descent Algorithm vs. adaptive boosting <br>
        <br>
          '비용함수(Cost Function)의 최소화'를 다른 방식으로 수행<br>
             Gradient Algorithm : 파라미터 조정<br>
             아다부스트 :  앙상블을 추가함으로써 <br>
    

# bias-variance trade-off<br>

  * 일반적으로 파라미터가 있는 알고리즘이 'High Bias' <br>
  
   -  모델이 빨리 배우고, 쉽게 이해하지만 '덜 felxible'하다.<br>
        > 파라미터가 없는 알고리즘은?<br>
        > 샘플로 모수를 추정하는 알고리즘(K-nearest Neighbor) 등등<br>

   - '복잡한 문제'에서 낮은 '예측률'을 보인다. <br>
      즉, 알고리즘의 bias(치우침)를 simplifying 하기 힘들다.<br>
      
    - 결정 트리는 "low bias"알고리즘 이다.<br>
      반면, 선형 회귀는 "high bias"이다.<br>

    
# "분산"<br>
  
    다른 샘플 데이터가 학습되었을 때, target function의 추정치가 얼마나 달라질지에 대한 지표<br>
    (target function : 트레이닝 데이터가 머신러닝으로 학습한 결과)<br>
    
    즉, 같은 데이터 셋(샘플)에 대해 variance 는 같아야 하고 0이 아니여야 한다.<br>
    
  * K-nearest Neighbors : High-variance <br>
    Discriminant Analysis : Low-variance<br>
<br>
  
 
# Machine learning 의 모델링의 목적 == "low Variance/ Bias"<br>
    Variance 와 Bias의 균형을 잘 맞추는게 관점!<br>
    
    Increasing the bias will decrease the variance.
    Increasing the variance will decrease the bias.

    
